# -*- coding: utf-8 -*-
"""ML PROJECT(Emotions)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mv7MAAZKlyViEqDW2kx3TLuW7Y14G9Tq

# **TEXT EMOTION DETECTION**
---

**Team Members:**

Hamna Mohiuddin - 21B-189-SE

Wareesha Ali - 21B-088-SE

Syed Owais Ali Shah - 21B-062-SE

# **IMPORTING NECESSARY LIBRARIES**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""**DATAFRAME FORMATION**"""

df=pd.read_csv("ml2.csv")

"""# **DATA EXPLORATION AND PREPROCESSING**

**Head() Function is used to print a few values of the dataset to understand it.**
"""

print(df.head(10))

"""**No# of Rows and Columns in the dataset.**"""

import pandas as pd
num_rows, num_columns = df.shape

print(f"Number of rows: {num_rows}")
print(f"Number of columns: {num_columns}")

"""**Names of all the columns.**"""

import pandas as pd
column_names = df.columns

print("Column names:")
print(column_names)

"""**Describe() is used here to find the statistics of the dataset.**"""

summary_statistics = df.describe()
print(summary_statistics)

"""**Duplicated is used on the dataset to find if there is any duplicated value in the dataset record.**"""

print(df.duplicated())

"""**Below isnull().sum() is showing if there is any null  value in any of the columns.**"""

print(df.isnull().sum())

"""**dropna() is removing all the null values in the dataset.**"""

df_cleaned = df.dropna()

print(df_cleaned.head())

"""**Duplicates is checking if there is any duplicated record in the dataset.**"""

df_no_duplicates = df.drop_duplicates(subset=['wid', 'hmid', 'reflection_period', 'original_hm', 'cleaned_hm',
       'modified', 'num_sentence', 'ground_truth_category',
       'predicted_category', 'age', 'country', 'gender', 'marital',
       'parenthood'])

print(df_no_duplicates.head())

"""**Forward Fill is used to handle the null values with the values placed before it.**"""

df.fillna(method="ffill",inplace=True)
df

"""# **DATA VISUALIZATION**

**Bar Graph here is showing the how frequently all the emotions occured.**
"""

category_counts = df['predicted_category'].value_counts()

plt.figure(figsize=(8, 6))
category_counts.plot(kind='bar', color='skyblue', edgecolor='black')
plt.xlabel('Predicted Category')
plt.ylabel('Frequency')
plt.title('Frequency of Predicted Categories')
plt.xticks(rotation=45)
plt.show()

"""**Pie Chart is showing the proportions all the sentiments carry in the dataset.**"""

import matplotlib.pyplot as plt

sentiment_counts = df['predicted_category'].value_counts()

plt.figure(figsize=(8, 8))
plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', colors=sns.color_palette('viridis'))
plt.title('Sentiment Proportions')
plt.show()

"""**Bar Graph here is showing the text length in each emotion.**"""

df['text_length_words'] = df['original_hm'].apply(lambda x: len(x.split()))
df['text_length_characters'] = df['original_hm'].apply(lambda x: len(x))

unique_categories = df['predicted_category'].unique()

num_subplots = len(unique_categories)

fig, axes = plt.subplots(1, num_subplots, figsize=(25, 10))

for i, category in enumerate(unique_categories):
    data_category = df[df['predicted_category'] == category]
    axes[i].hist(data_category['text_length_words'], bins=15, color='skyblue', alpha=0.7, edgecolor='black')
    axes[i].set_title(f'Text Length (Words) for {category.capitalize()}')
    axes[i].set_xlabel('Number of Words')
    axes[i].set_ylabel('Frequency')

plt.tight_layout()
plt.show()

"""**Bar Graph here is showing how frequently some words occur in each emotion.**"""

from collections import Counter
import re

def count_words_in_category(df, category):
    texts = df[df['predicted_category'] == category]['original_hm']
    all_text = " ".join(texts)
    words = re.findall(r'\b\w+\b', all_text.lower())
    return Counter(words)

def plot_word_frequency(df):
    categories = df['predicted_category'].unique()
    for category in categories:
        word_counts = count_words_in_category(df, category)
        common_words = word_counts.most_common(10)
        df_common_words = pd.DataFrame(common_words, columns=['Word', 'Frequency'])
        plt.figure(figsize=(10, 6))
        plt.bar(df_common_words['Word'], df_common_words['Frequency'], color='skyblue')
        plt.xlabel('Words')
        plt.ylabel('Frequency')
        plt.title(f'Top 10 Words in {category.capitalize()}')
        plt.xticks(rotation=45)
        plt.show()

plot_word_frequency(df)

!pip install vaderSentiment

"""**VaderSentiment is a library that calculates the sentiment score of text. Here VaderSentiment is used to calculate score and then find out correlation between all the columns.**"""

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()

def get_sentiment_scores(text):
    return analyzer.polarity_scores(text)['compound']

df['compound_sentiment'] = df['cleaned_hm'].apply(get_sentiment_scores)

numeric_df = df.select_dtypes(include=['int64', 'float64'])

correlation_matrix = numeric_df.corr()

print("Correlation Matrix:")
print(correlation_matrix['compound_sentiment'])

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

!pip install neattext

"""**Neattext library helps in cleaning the data. Here it is used to remove usernames from the text.**"""

import neattext.functions as nfx

# Remove the user handles
df['cleaned_hm'] = df['cleaned_hm'].apply(nfx.remove_userhandles)

dir(nfx)

"""**Here stopwords like 'is', 'am', 'are', etc. are removed from the data since they contain no significant information.**"""

# Remove the stopwords
df['cleaned_hm'] = df['cleaned_hm'].apply(nfx.remove_stopwords)

"""**Breaking the dataset into Features and Target. Where x is feature and y is target.**"""

x = df['cleaned_hm']
y = df['predicted_category']

"""**Train Test Split is used here to breakdown dataset into training data and testing data.**"""

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)

"""# **MODEL APPLICATION**

"""

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

"""**Logistic Regression is used here since the data is textual and classification based. Logistic Regression works well with Multi-class classification and sparse data. Moreover, it also helps in avoiding overfitting.**"""

pipe_lr = Pipeline(steps=[('cv',CountVectorizer()),('lr',LogisticRegression())])
pipe_lr.fit(x_train,y_train)
pipe_lr.score(x_test,y_test)

"""**Another model, SVM is used here since  it can handle high-dimensional data, is effective in cases with limited training samples.**"""

pipe_svm = Pipeline(steps=[('cv',CountVectorizer()),('svc', SVC(kernel = 'rbf', C = 10))])
pipe_svm.fit(x_train,y_train)
pipe_svm.score(x_test,y_test)

"""**Random Forest Classifier also avoid overfitting, and takes decision by creating multiple trees and their average. It is also computationally efficient and less time consuming.**"""

pipe_rf = Pipeline(steps=[('cv',CountVectorizer()),('rf', RandomForestClassifier(n_estimators=10))])
pipe_rf.fit(x_train,y_train)
pipe_rf.score(x_test,y_test)

"""# **Accuracy Report**"""

rf_pred = pipe_rf.predict(x_test)
lr_pred = pipe_lr.predict(x_test)
svm_pred = pipe_svm.predict(x_test)
# Logistic Regression metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
print("Logistic Regression:")
print("Accuracy:", accuracy_score(y_test, lr_pred))
print("Precision:", precision_score(y_test, lr_pred, average='weighted'))
print("Recall:", recall_score(y_test, lr_pred, average='weighted'))
print("F1-Score:", f1_score(y_test, lr_pred, average='weighted'))
print(classification_report(y_test, lr_pred))

# SVM metrics
print("\nSVM:")
print("Accuracy:", accuracy_score(y_test, svm_pred))
print("Precision:", precision_score(y_test, svm_pred, average='weighted'))
print("Recall:", recall_score(y_test, svm_pred, average='weighted'))
print("F1-Score:", f1_score(y_test, svm_pred, average='weighted'))
print(classification_report(y_test, svm_pred))

# Random Forest metrics
print("\nRandom Forest:")
print("Accuracy:", accuracy_score(y_test, rf_pred))
print("Precision:", precision_score(y_test, rf_pred, average='weighted'))
print("Recall:", recall_score(y_test, rf_pred, average='weighted'))
print("F1-Score:", f1_score(y_test, rf_pred, average='weighted'))
print(classification_report(y_test, rf_pred))

"""**The Final Model chosen is Logistic Regression since it has the best accuracy on the dataset of 83%.**"""

import joblib
pipeline_file = open("text_emotion.pkl","wb")
joblib.dump(pipe_lr,pipeline_file)
pipeline_file.close()

