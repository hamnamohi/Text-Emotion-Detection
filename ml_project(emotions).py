# -*- coding: utf-8 -*-
"""ML PROJECT(Emotions)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mv7MAAZKlyViEqDW2kx3TLuW7Y14G9Tq
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv("ml2.csv")

print(df.isnull().sum())

print(df.head(10))

print(df.duplicated())

import pandas as pd
num_rows, num_columns = df.shape

print(f"Number of rows: {num_rows}")
print(f"Number of columns: {num_columns}")

import pandas as pd
column_names = df.columns

print("Column names:")
print(column_names)

df_cleaned = df.dropna()

print(df_cleaned.head())

df_no_duplicates = df.drop_duplicates(subset=['wid', 'hmid', 'reflection_period', 'original_hm', 'cleaned_hm',
       'modified', 'num_sentence', 'ground_truth_category',
       'predicted_category', 'age', 'country', 'gender', 'marital',
       'parenthood'])

print(df_no_duplicates.head())

df.fillna(method="ffill",inplace=True)
df

import pandas as pd
num_rows, num_columns = df.shape

print(f"Number of rows: {num_rows}")
print(f"Number of columns: {num_columns}")

summary_statistics = df.describe()
print(summary_statistics)

category_counts = df['predicted_category'].value_counts()

plt.figure(figsize=(8, 6))
category_counts.plot(kind='bar', color='skyblue', edgecolor='black')
plt.xlabel('Predicted Category')
plt.ylabel('Frequency')
plt.title('Frequency of Predicted Categories')
plt.xticks(rotation=45)
plt.show()

import matplotlib.pyplot as plt

sentiment_counts = df['predicted_category'].value_counts()

plt.figure(figsize=(8, 8))
plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', colors=sns.color_palette('viridis'))
plt.title('Sentiment Proportions')
plt.show()

df['text_length_words'] = df['original_hm'].apply(lambda x: len(x.split()))
df['text_length_characters'] = df['original_hm'].apply(lambda x: len(x))

unique_categories = df['predicted_category'].unique()

num_subplots = len(unique_categories)

fig, axes = plt.subplots(1, num_subplots, figsize=(25, 10))

for i, category in enumerate(unique_categories):
    data_category = df[df['predicted_category'] == category]
    axes[i].hist(data_category['text_length_words'], bins=15, color='skyblue', alpha=0.7, edgecolor='black')
    axes[i].set_title(f'Text Length (Words) for {category.capitalize()}')
    axes[i].set_xlabel('Number of Words')
    axes[i].set_ylabel('Frequency')

plt.tight_layout()
plt.show()

from collections import Counter
import re

def count_words_in_category(df, category):
    texts = df[df['predicted_category'] == category]['original_hm']
    all_text = " ".join(texts)
    words = re.findall(r'\b\w+\b', all_text.lower())
    return Counter(words)

def plot_word_frequency(df):
    categories = df['predicted_category'].unique()
    for category in categories:
        word_counts = count_words_in_category(df, category)
        common_words = word_counts.most_common(10)
        df_common_words = pd.DataFrame(common_words, columns=['Word', 'Frequency'])
        plt.figure(figsize=(10, 6))
        plt.bar(df_common_words['Word'], df_common_words['Frequency'], color='skyblue')
        plt.xlabel('Words')
        plt.ylabel('Frequency')
        plt.title(f'Top 10 Words in {category.capitalize()}')
        plt.xticks(rotation=45)
        plt.show()

plot_word_frequency(df)

!pip install vaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()

def get_sentiment_scores(text):
    return analyzer.polarity_scores(text)['compound']

df['compound_sentiment'] = df['cleaned_hm'].apply(get_sentiment_scores)

numeric_df = df.select_dtypes(include=['int64', 'float64'])

correlation_matrix = numeric_df.corr()

print("Correlation Matrix:")
print(correlation_matrix['compound_sentiment'])

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

import pandas as pd
import numpy as np
import seaborn as sns

!pip install neattext

import neattext.functions as nfx

# Remove the user handles
df['Clean_Text'] = df['cleaned_hm'].apply(nfx.remove_userhandles)

dir(nfx)

# Remove the stopwords
df['cleaned_hm'] = df['cleaned_hm'].apply(nfx.remove_stopwords)

x = df['cleaned_hm']
y = df['predicted_category']

from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

pipe_lr = Pipeline(steps=[('cv',CountVectorizer()),('lr',LogisticRegression())])
pipe_lr.fit(x_train,y_train)
pipe_lr.score(x_test,y_test)

pipe_svm = Pipeline(steps=[('cv',CountVectorizer()),('svc', SVC(kernel = 'rbf', C = 10))])
pipe_svm.fit(x_train,y_train)
pipe_svm.score(x_test,y_test)

import joblib
pipeline_file = open("svc.pkl", "wb")
joblib.dump(pipe_svm, pipeline_file)
pipeline_file.close()

!pip install neattext

pipe_rf = Pipeline(steps=[('cv',CountVectorizer()),('rf', RandomForestClassifier(n_estimators=10))])
pipe_rf.fit(x_train,y_train)
pipe_rf.score(x_test,y_test)

rf_pred = pipe_rf.predict(x_train)
lr_pred = pipe_lr.predict(x_train)
svm_pred = pipe_svm.predict(x_train)
# Logistic Regression metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
print("Logistic Regression:")
# print("Accuracy:", accuracy_score(y_test, lr_pred))
print("Precision:", precision_score(y_test, lr_pred, average='weighted'))
print("Recall:", recall_score(y_test, lr_pred, average='weighted'))
print("F1-Score:", f1_score(y_test, lr_pred, average='weighted'))
print(classification_report(y_test, lr_pred))

# SVM metrics
print("\nSVM:")
# print("Accuracy:", accuracy_score(y_test, svm_pred))
print("Precision:", precision_score(y_test, svm_pred, average='weighted'))
print("Recall:", recall_score(y_test, svm_pred, average='weighted'))
print("F1-Score:", f1_score(y_test, svm_pred, average='weighted'))
print(classification_report(y_test, svm_pred))

# Random Forest metrics
print("\nRandom Forest:")
# print("Accuracy:", accuracy_score(y_test, rf_pred))
print("Precision:", precision_score(y_test, rf_pred, average='weighted'))
print("Recall:", recall_score(y_test, rf_pred, average='weighted'))
print("F1-Score:", f1_score(y_test, rf_pred, average='weighted'))
print(classification_report(y_test, rf_pred))



import joblib
pipeline_file = open("text_emotion.pkl","wb")
joblib.dump(pipe_lr,pipeline_file)
pipeline_file.close()

